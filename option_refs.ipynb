{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed2e9f39",
   "metadata": {},
   "source": [
    "Understanding Premium-Adjusted Delta and Strike Inversion\n",
    "In FX option markets, premium-adjusted delta is a delta convention that accounts for the option premium being paid in a particular currency (often the foreign currency). It modifies the standard Black–Scholes delta by subtracting the premium term, effectively reducing the delta by an amount proportional to the option’s price. For example, the premium-adjusted spot delta for a call can be written (in one common convention) as: \n",
    "Δ\n",
    "𝑆\n",
    ",\n",
    "pa\n",
    "(\n",
    "𝐾\n",
    ")\n",
    "=\n",
    "𝜙\n",
    " \n",
    "𝑒\n",
    "−\n",
    "𝑟\n",
    "𝑑\n",
    "𝑇\n",
    " \n",
    "𝐾\n",
    "𝑆\n",
    " \n",
    "𝑁\n",
    "(\n",
    "𝜙\n",
    " \n",
    "𝑑\n",
    "2\n",
    ")\n",
    ",\n",
    "Δ \n",
    "S,pa\n",
    "​\n",
    " (K)=ϕe \n",
    "−r \n",
    "d\n",
    "​\n",
    " T\n",
    "  \n",
    "S\n",
    "K\n",
    "​\n",
    " N(ϕd \n",
    "2\n",
    "​\n",
    " ), where $\\phi = +1$ for a call (–1 for a put), $r_d$ is the domestic interest rate, $T$ is time to maturity, $S$ is spot, $K$ the strike, and $N(\\cdot)$ the standard normal CDF\n",
    "researchgate.net\n",
    ". This formula shows that when the premium is paid in the foreign currency, the delta uses $N(d_2)$ (associated with strike) instead of the usual $N(d_1)$ term. The premium-adjusted delta is a key input in FX smile construction – quotes like “25Δ call” are often given in terms of this delta convention, and one must invert this relationship to solve for the strike $K$ corresponding to a given delta quote. However, unlike standard delta, the mapping from strike to premium-adjusted delta is not one-to-one for calls\n",
    "researchgate.net\n",
    ". The function $\\Delta_{S,\\text{pa}}(K)$ is non-monotonic in $K$ for calls (with a single-peaked shape), meaning a given delta value can correspond to two different strikes (an in-the-money and an out-of-the-money strike)\n",
    "researchgate.net\n",
    "researchgate.net\n",
    ". In contrast, for puts the premium-adjusted delta is monotonic in strike (decreasing smoothly), so inversion is straightforward\n",
    "researchgate.net\n",
    ". This fundamental difference leads to numerical difficulties when solving for strike from a premium-adjusted call delta. Illustrative behavior: As $K \\to 0$ (deep ITM call), the call delta approaches 1 but the premium-adjusted call delta tends toward 0 because the option’s premium is almost as large as the spot itself, canceling out the delta hedge. As $K \\to \\infty$ (deep OTM), the call delta (and premium-adjusted delta) both approach 0. In between, $\\Delta_{\\text{pa}}(K)$ rises to a maximum at some intermediate strike (often near the ATM region) and then falls off\n",
    "researchgate.net\n",
    ". This means the equation $\\Delta_{S,\\text{pa}}(K) = \\Delta_{\\text{target}}$ can have two solutions (two strikes) if $\\Delta_{\\text{target}}$ is below the peak value of the function. If $\\Delta_{\\text{target}}$ exceeds the maximum achievable premium-adjusted delta, no real strike can produce that delta (no solution)\n",
    "researchgate.net\n",
    ". Figure 1 (Panel A) conceptually illustrates this hump: the premium-adjusted call delta (solid curve) increases and then decreases, crossing a given level (dashed line) twice (two possible strikes), whereas the standard call delta (monotonic, dotted curve) crosses only once. Numerical implication: Root-finding algorithms assume a well-behaved function (usually monotonic) bracketing a single root. The non-monotonic, bell-shaped nature of $\\Delta_{\\text{pa}}(K)$ violates these assumptions. This often causes failures or inaccuracies in numerical solvers like Brent’s method or secant method when computing strike from a given delta. Below, we detail the causes of failure and compare methods.\n",
    "Why Root-Finding Can Fail for Premium-Adjusted Delta\n",
    "Monotonicity and Multiple Roots: The primary mathematical cause of failure is the lack of monotonicity. Brent’s bracketing algorithms (Brent’s method, bisection, Ridder’s, etc.) require the function values at the bracket endpoints to have opposite signs, ensuring a single root in between. If one naively brackets the entire range of possible strikes (e.g. from a very low $K$ up to a very high $K$), the premium-adjusted delta at both ends might be below the target (e.g. $\\Delta_{\\text{pa}}(K_{\\min}) < \\Delta_{\\text{target}}$ and $\\Delta_{\\text{pa}}(K_{\\max}) < \\Delta_{\\text{target}}$)\n",
    "researchgate.net\n",
    ". This happens if $\\Delta_{\\text{target}}$ lies below the peak of the function – the function $\\Delta_{\\text{pa}}(K)$ starts below target, rises above it (making $f(K)=\\Delta_{\\text{pa}}- \\Delta_{\\text{target}}$ positive) then falls below it again by the far end. The net change from one end to the other does not guarantee a sign flip, violating Brent’s bracketing condition. In such cases, Brent’s method will immediately fail with an error (“f(a) and f(b) must have different signs”) because the root is not bracketed in a simple way. Moreover, if the solver is not carefully guided to the correct branch, it may converge to the undesired root (e.g. the ITM strike instead of the OTM strike). Market convention dictates using the OTM solution (for calls, the higher strike)\n",
    "quant.stackexchange.com\n",
    ". Thus, one must restrict the search to the appropriate side of the delta hump (the “right-hand side” of the maximum for calls)\n",
    "quant.stackexchange.com\n",
    "researchgate.net\n",
    ". Searching across the entire domain without this restriction can confuse the root-finder or yield the wrong strike. Flat extrema and slow convergence: Even if the root-finder’s domain is restricted to one side of the peak, the vicinity of the peak itself is problematic. At the maximum of $\\Delta_{\\text{pa}}(K)$, the derivative $\\mathrm{d}\\Delta_{\\text{pa}}/\\mathrm{d}K = 0$. If the target delta is near this maximum, the solver may struggle:\n",
    "Secant or Newton’s method could land near the flat slope where the function changes very slowly with $K$. The secant update may produce a huge jump (or Newton’s update may blow up due to $f'(K)\\approx0$), causing divergence or a step outside the valid domain (negative or zero strike, which is not defined due to $\\ln(K)$ in Black–Scholes). A domain error can occur if, say, Newton’s method computes a negative next iterate (e.g. taking a step that crosses $K=0$ where the function is undefined).\n",
    "Even Brent’s method (which switches to bisection when needed) can suffer slow convergence near a very flat region. It will eventually converge, but might require many iterations if the function is almost horizontal near the root, since bisection makes slow progress when the function values are nearly equal on both sides.\n",
    "No root or out-of-range: If the target premium-adjusted delta is not attainable (e.g. higher than the maximum, or for a call maybe a very high delta that only the ITM branch could yield), the solver might iterate without finding a sign change. In practice, a well-designed routine will detect this by evaluating the function at the peak or at logical bounds:\n",
    "For instance, the maximum premium-adjusted call delta might be, say, 0.75 in some scenario\n",
    "researchgate.net\n",
    ". If asked for a strike with delta 0.80, no solution exists. Brent’s method would fail to bracket a root at all (both ends give $f<0$), and Newton may wander off without convergence. The correct approach is to recognize this condition (e.g. compare target vs theoretical max) and handle it (return an error or clamp to the nearest feasible delta).\n",
    "In summary, non-monotonicity is the key mathematical issue\n",
    "researchgate.net\n",
    ". It leads to multiple roots and flat spots that violate assumptions of standard root-finding methods. Without special care (like restricting the domain or providing a good initial guess), methods like brentq or secant can fail to converge or converge to an incorrect value.\n",
    "Comparing Root-Finding Algorithms and Their Applicability\n",
    "Different root-finding algorithms have varied robustness and speed characteristics. Below we review them in the context of this problem, highlighting when each is appropriate:\n",
    "Bisection method: This simple bracketing method always converges linearly to a root if you can provide an interval [a, b] with a sign change ($f(a)f(b)<0$). It is very robust (it will not diverge or overshoot the root), but relatively slow. In the strike-from-delta problem, if you isolate the correct branch and can find such an interval bracketing the strike, bisection will succeed. However, bisection might require many iterations for high precision because it halves the interval each step. It’s a good fallback when fancy methods fail.\n",
    "Secant method: The secant method uses two initial estimates and iteratively computes roots of the secant line. It has faster convergence than bisection (super-linear, approximately the golden ratio $\\approx1.62$ order)\n",
    "researchgate.net\n",
    " but is not guaranteed to converge. Without a bracket, secant can diverge or wander if the function is not well-behaved. In our case, if the initial guesses straddle the peak or are on the wrong side, secant might jump into an invalid region (negative $K$) or converge to the wrong root. It works best when you have a good idea where the root is (e.g. you’ve pre-identified the OTM strike region). Given the complexities here, a pure secant approach can be unreliable.\n",
    "Newton–Raphson method: Newton’s method uses the function and its derivative $f'(x)$ to iteratively refine a single guess: $x_{n+1} = x_n - f(x_n)/f'(x_n)$. When applicable, it converges quadratically fast. But it requires computing $f'(K)$ (here, the derivative of delta w.r.t. strike) and a sufficiently good starting guess. In the strike inversion, an analytic $f'(K)$ can be derived (though it’s a bit complex), or one can compute it via automatic differentiation. Newton’s method can fail dramatically if started too far from the root – for example, if started near the delta peak where $f' \\approx 0$, the Newton step $\\Delta K \\approx -f/f'$ becomes huge. It could jump to a nonsensical negative strike or oscillate. Thus, Newton is powerful when initialized close to the correct root (perhaps using another method’s coarse solution), but on its own it needs safeguards (like step size limiting or fallback to bisection if out-of-bounds).\n",
    "Brent’s method (Brent–Dekker): Brent’s method is a hybrid algorithm combining bisection, secant, and inverse quadratic interpolation. It is often the default choice for robust root-finding because it is guaranteed to converge (like bisection) as long as the root is bracketed, and it exploits faster secant-like steps when possible\n",
    "researchgate.net\n",
    "researchgate.net\n",
    ". Brent’s method has order ~1.84 convergence in practice\n",
    "researchgate.net\n",
    " – faster than bisection, though not as fast as pure Newton. In the context of FX delta inversion, Brent’s method is recommended in literature as a robust approach. Reiswich and Wystup (2012) specifically suggest using Brent’s algorithm for solving the strike from delta in smile construction. The caveat is that one must supply a valid bracketing interval on the correct branch. Brent’s method fails in this case primarily if the user does not bracket the right root (e.g. the function doesn’t have a simple sign change between the chosen endpoints, as discussed). If given a proper interval on the monotonic portion of the delta curve (e.g. from slightly below the peak to a high strike), Brent’s method will safely converge to the desired strike. The failure widely reported with Brent’s algorithm for premium-adjusted delta usually stems from bracketing the wrong region or the function’s double-root behavior, not from a flaw in Brent’s method itself.\n",
    "Ridder’s method and TOMS 748: These are other bracketed root-finding algorithms. Ridder’s method uses an exponential fitting to converge faster than bisection while preserving bracketing; TOMS Algorithm 748 is another hybrid method (like Brent’s) with efficient bracket convergence. Both require an initial bracket with opposite signs. Their performance is similar to Brent’s in robustness. They could be used as alternatives if Brent’s method fails to converge in a reasonable number of iterations (though that would typically indicate a bracketing issue rather than algorithm pathology). In practice, switching from Brent to Ridder or TOMS748 will not magically solve the non-monotonicity problem – the key is still to isolate a single root.\n",
    "Why Brent (or secant) fails here: Summarizing the above, Brent’s method fails not due to a poor algorithm but because of how the problem violates its requirements. If you call scipy.optimize.brentq or brent without carefully choosing the interval, you may get an exception or an incorrect root:\n",
    "If $f(K_{\\min})$ and $f(K_{\\max})$ are not of opposite sign (e.g. both negative), Brent’s algorithm will refuse to start\n",
    "researchgate.net\n",
    ". This is a common outcome when one uses overly broad bounds for the premium-adjusted call delta case.\n",
    "If one mistakenly brackets across the peak (so that the function goes up and then down within the interval), there are multiple sign changes inside. Brent’s method will still only find one root – typically the first one it encounters when scanning from one end. This could be the ITM strike solution rather than the desired OTM solution, if the interval wasn’t chosen carefully to target the correct crossing.\n",
    "The secant method, if used in SciPy via root_scalar(method='secant') or similar, can fail by diverging or zero-dividing if two successive iterates land on nearly equal function values (possible near the flat top) or if it jumps outside the domain. Without the bracketing safety net, secant is even more sensitive to these issues.\n",
    "It’s worth noting that even when these methods converge, precision issues can arise. The strike as a function of delta can be extremely sensitive near the peak – a small change in quoted delta can lead to a large move in strike. As a result, if the root-finder stops with only moderate precision, the computed strike may oscillate when inputs are perturbed by tiny amounts. Jäckel (2020) observed that using Brent with loose tolerances led to noisy Greeks in risk reports: small bumps in implied vol or spot caused the inverted strike (and thus vega, gamma) to jump around\n",
    "researchgate.net\n",
    ". Brent’s default convergence (order ~1.84) was not sufficient for “full attainable precision” in that context\n",
    "researchgate.net\n",
    "researchgate.net\n",
    ". This underlines that while Brent is robust, one may need to tighten tolerances or use higher-precision methods to get stable results for downstream sensitivities.\n",
    "Strategies for Robust Convergence (Avoiding Non-Convergence)\n",
    "To reliably invert the premium-adjusted delta, practitioners employ several strategies to handle the issues above:\n",
    "Bracket the correct root on the right branch: The common solution is to restrict the search to the out-of-the-money strike for calls (i.e. the higher strike corresponding to the delta)\n",
    "quant.stackexchange.com\n",
    "researchgate.net\n",
    ". In practical terms, one finds an interval [a, b] that lies entirely on the descending part of the delta curve (right of the peak). One way is:\n",
    "Compute the strike that gives the same non-premium-adjusted delta. For example, if target $\\Delta_{\\text{pa}}=0.25$, find $K_{0.25}$ such that standard delta = 0.25 (this can be done analytically via the inverse Black–Scholes formula since standard call delta is monotonic). This $K_{0.25}$ is an upper bound – the premium-adjusted delta for a given strike is always smaller than the regular delta for that strike\n",
    "researchgate.net\n",
    ". So at $K=K_{0.25}$, $\\Delta_{\\text{pa}} < 0.25$ (i.e. $f(K_{0.25})<0$).\n",
    "Use a point near the delta peak as the lower bound. Often the ATM strike (forward rate) is near the peak; one can start with $a = K_{\\text{ATM-forward}}$ or a bit below it. At ATM, premium-adjusted call delta is typically around 0.5 (less if premium-adjusted), so $f(a) = \\Delta_{\\text{pa}}(a) - \\Delta_{\\text{target}}$ will likely be positive if target is 0.25 (since $\\Delta_{\\text{pa}}(ATM) > 0.25$ for most cases). Indeed, the literature advises to search for strikes “on the right side of the delta maximum” – effectively bracketing the root between the peak and a high strike\n",
    "researchgate.net\n",
    ".\n",
    "Now $f(a)>0$ (at or just past the peak) and $f(b)<0$ (far out where delta falls below target), satisfying the sign change condition. Brent’s method or bisection can safely converge in [a, b] to the higher strike solution. This ensures we find the OTM strike (the intended one) rather than the lower strike. This approach is explicitly recommended by Wystup and colleagues\n",
    "researchgate.net\n",
    ".\n",
    "Detect multiple roots or no root: If the function is evaluated and found not to bracket a root, it’s wise to check the delta maximum. One can compute the peak premium-adjusted delta by setting derivative=0 or by simply evaluating $\\Delta_{\\text{pa}}$ at ATM and a bit around to locate the maximum. If the target delta is greater than the peak, the solver should abort with a clear message (or clamp the result). This prevents futile iterations. If two roots are theoretically possible (target below peak), the code should consistently choose one. By convention, pick the OTM for calls (and the OTM for puts, which in that case is just the only root since put $\\Delta_{\\text{pa}}$ is monotonic). In practice, always assume one root by construction: e.g. “25Δ call” is implicitly the OTM call. The other root (ITM strike) is ignored as an extraneous solution.\n",
    "Use a two-step approach if needed: A robust procedure might first solve for the delta peak (or at least bracket it) and then do two separate root finds: one on each side of the peak if needed. This is rarely necessary in practice because of the quoting conventions (you know which side to use), but it’s a safeguard. For example, one could:\n",
    "Find the strike $K_{\\max\\Delta}$ where $\\Delta_{\\text{pa}}$ is maximum (e.g. via Newton on $\\partial \\Delta/\\partial K=0$ or simple line search).\n",
    "If $\\Delta_{\\text{target}}$ is below $\\Delta_{\\text{pa}}(K_{\\max\\Delta})$, then there are two possible roots. Determine which root is relevant (OTM or ITM) by context and solve in that region only.\n",
    "If needed (for completeness), one could solve for both strikes (one below $K_{\\max\\Delta}$, one above) and compare. But again, typically the market only cares about one of them.\n",
    "Fallback to bisection if other methods falter: If a hybrid method (Brent) fails to converge due to some numerical quirk (e.g. extremely flat $f$ requiring more iterations than allowed), a brute-force bisection with a high iteration cap can be employed on the known bracket. Bisection will inch its way to the root regardless of function slope. One can also tighten the convergence criteria (tolerance) to ensure the strike is found to high precision, avoiding the Greeks noise issue. This of course costs more iterations, but strike finding is usually not the bottleneck in smile construction since it’s one-dimensional.\n",
    "Limit step size or enforce domain in open methods: For methods like Newton or secant, implementing simple safeguards improves robustness:\n",
    "If Newton’s update proposes $K_{n+1} < 0$, one can half the step or reset $K_{n+1}$ to a small positive number (and perhaps switch to bisection).\n",
    "If the update overshoots past the bracket (if you maintain a bracket during Newton iterations), bring it back into the interval (the “Illinois” or “pegasus” modifications to secant do this). Brent’s method internally does such safeguarding automatically – it will revert to bisection if the interpolation goes out of range.\n",
    "Set a maximum number of iterations. If exceeded, either refine the bracket or report non-convergence.\n",
    "Ensure continuity of the function implementation: The premium-adjusted delta formula itself is continuous for $K>0$. But numerical implementation should avoid any discontinuities (e.g. from using approximations). Using a high-quality normal CDF function (or math library) is important to get smooth outputs. Any discontinuity in $f(K)$ would break root-finders. Typically, this is not an issue since $N(d_2)$ is smooth, but one should avoid low-precision hacks that might introduce kinks.\n",
    "By applying these strategies, one can significantly reduce the chances of solver failure. For instance, Reiswich & Wystup (2010) illustrate the approach of targeting the right-hand root for the call delta inversion and then using Brent’s method on a safe interval\n",
    "researchgate.net\n",
    ". Following such guidance, the strike inversion becomes a stable part of the smile calibration process.\n",
    "Alternative Robust Root-Finding Methods and Tools\n",
    "Beyond the basic algorithms, there are practical tools and modern techniques to improve robustness:\n",
    "SciPy’s root_scalar with advanced methods: The SciPy library in Python provides a unified interface root_scalar where you can choose methods like 'brentq', 'brenth' (Brent–Dekker), 'ridder', 'bisect', 'secant', or 'toms748'. All bracketed methods (brentq, brenth, ridder, bisect, toms748) will reliably find a root in an interval if you supply a valid bracket. For our problem, brentq or toms748 are good choices once the interval is set up, as they are efficient and robust. The 'toms748' method, for example, performs at most four function evaluations per iteration and converges super-linearly; it can sometimes outperform Brent’s method in practice. If bracketing is tricky, one could combine bisect (to bracket step by step) with one of these faster methods: e.g. first use a small grid search to find a sign change, then call brentq on that bracket.\n",
    "Using high-precision arithmetic: Libraries like mpmath can perform root-finding with arbitrary precision arithmetic. If numerical precision is a concern (e.g. for very flat curves or to reduce noise in Greeks), one can increase the working precision. Mpmath’s findroot function can do Newton’s method or secant with high precision. This can mitigate issues where double precision floating-point errors might cause a miss (though typically double precision is fine for delta inversion).\n",
    "Symbolic or semi-analytic approaches: In some cases, one can manipulate the equation to solve it more directly:\n",
    "Symbolic math (Sympy): The equation $\\phi e^{-r_d T}(K/S)N(\\phi d_2) = \\Delta_0$ does not have a closed-form algebraic solution due to $N(d_2)$ (which involves an error function). Sympy cannot solve it analytically in elementary terms. However, one can use Sympy’s nsolve to solve it numerically, which essentially wraps a Newton-like method. That wouldn’t fundamentally change the convergence issues discussed unless you provide a good initial guess.\n",
    "Custom analytic inversion: Notably, researchers have derived semi-analytic inversion formulas. Jäckel (2020) proposed an analytical two-step procedure to get $K$ from $\\Delta_{\\text{pa}}$\n",
    "researchgate.net\n",
    ". The idea is to change variables to solve for the normal variate $y = d_2$ that satisfies a transformed equation, then recover $K$ from $y$. By applying the inverse error function and some algebra, one can avoid iterative root-finding altogether. The result is a closed-form (or effectively closed-form) expression for $y$ (hence $K$) in terms of $\\Delta_{\\text{pa}}$ and other parameters\n",
    "researchgate.net\n",
    "researchgate.net\n",
    ". Such formulas are beyond standard libraries, but if available, they are extremely fast and precise. They eliminate numerical instabilities by design. So, if one is implementing a production system and worried about Brent’s moderate convergence and potential noise, exploring these analytic solutions is worthwhile. They are documented in the literature\n",
    "researchgate.net\n",
    " and provide a robust fallback: essentially, bypassing root-finding by solving the math more directly.\n",
    "Automatic differentiation (AD) for Newton’s method: Tools like JAX, TensorFlow, or PyTorch can compute derivatives of functions automatically. You can leverage this for a custom Newton solver. For example, using JAX one could define a function for $\\Delta_{\\text{pa}}(K)$ and use jax.grad to get $d(\\Delta_{\\text{pa}})/dK`. Then implement a few Newton iterations manually. The benefit is you don’t have to derive the complex derivative by hand, and you retain high precision in the derivative computation. This still requires a decent initial guess and safeguards, but it can converge very fast if used on the correct branch. An advantage of AD-based solvers is that they can be vectorized (solve for many strikes in parallel) and easily integrated into machine learning or calibration pipelines, which leads to the next point.\n",
    "Root-finding via optimization (PyTorch example): We can frame the root-finding as an optimization: find $K$ that minimizes the error $f(K)^2 = (\\Delta_{\\text{pa}}(K) - \\Delta_{\\text{target}})^2$. Modern libraries don’t have a black-box root solver, but they have optimizers that can minimize functions. For example, one could use PyTorch’s autograd and optimizers:\n",
    "Define $K$ as a learnable tensor with requires_grad=True, initialize it (e.g. at the ATM strike).\n",
    "Define the loss $L(K) = (\\Delta_{\\text{pa}}(K) - \\Delta_{\\text{target}})^2$.\n",
    "Run a few steps of a gradient-based optimizer (like Adam or LBFGS) to minimize $L$. The gradient $\\partial L/\\partial K = 2(\\Delta_{\\text{pa}} - \\Delta_{\\text{target}})\\partial \\Delta_{\\text{pa}}/\\partial K$ is computed via autograd. When $L$ is driven to (near) zero, $\\Delta_{\\text{pa}}(K)$ equals the target. Essentially, this is finding the root of $f(K)$.\n",
    "PyTorch’s LBFGS optimizer is a good choice for this task; it’s a quasi-Newton method that can find zeros of $f$ quite efficiently if well-initialized. The code would look roughly like:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
